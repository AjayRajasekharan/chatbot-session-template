{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b26590",
   "metadata": {},
   "source": [
    "# üß† In-Class Exercise: Building Your First LLM Chatbot\n",
    "\n",
    "Welcome!  \n",
    "This notebook is your hands-on lab for **Session 2 ‚Äì Introduction to LLM Chatbots**.  \n",
    "You‚Äôll go step-by-step through concepts we discussed in class ‚Äî pipelines, parameters, and model behavior ‚Äî and try small experiments to understand how LLMs actually ‚Äúthink.‚Äù  \n",
    "\n",
    "üëâ Each section follows this pattern:\n",
    "- **Mini Concept** (theory recap)  \n",
    "- **Example Block** (run + observe)  \n",
    "- **Task Block** (guided exercise)  \n",
    "- **Challenge Block** (think deeper / explore)  \n",
    "\n",
    "Let‚Äôs get started üöÄ  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad1381",
   "metadata": {},
   "source": [
    "## üß© Concept 1: The Hugging Face Pipeline\n",
    "\n",
    "**Theory Recap:**  \n",
    "A pipeline is like a ‚Äúready-made tool‚Äù that connects your text input to an AI model.  \n",
    "Instead of manually loading weights and tokenizers, we use a *pipeline* for common tasks such as summarization, translation, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261d628",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Create a simple pipeline and use it\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Choose your task and model\n",
    "task = \"text2text-generation\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Step 2: Create the pipeline\n",
    "generator = pipeline(task, model=model_name)\n",
    "\n",
    "# Step 3: Try it out\n",
    "response = generator(\"Summarize: Artificial intelligence helps automate tasks.\")\n",
    "print(\"Output:\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0e016",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 1 (Guided Practice)\n",
    "# Use a different model - distilgpt2\n",
    "# 1. Change the task to \"text-generation\"\n",
    "# 2. Use model_name = \"distilgpt2\"\n",
    "# 3. Create your own prompt like \"Once upon a time...\"\n",
    "\n",
    "# Your code below üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f54210",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 1 (Critical Thinking)\n",
    "# What happens if you use the WRONG task for a model?\n",
    "# Try using \"text2text-generation\" instead of \"text-generation\" for distilgpt2.\n",
    "# Does it throw an error or produce something odd?\n",
    "# Write your observation in a comment below üëá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8efdf",
   "metadata": {},
   "source": [
    "## üß© Concept 2: Controlling Model Creativity\n",
    "\n",
    "**Theory Recap:**  \n",
    "Parameters like `temperature`, `top_p`, and `max_new_tokens` control how ‚Äúcreative‚Äù or ‚Äúfocused‚Äù the model‚Äôs output is.  \n",
    "- **Temperature**: randomness (0 = deterministic, 1 = more creative).  \n",
    "- **Top-p**: diversity of words considered.  \n",
    "- **Max new tokens**: how long the response can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4af881",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Comparing low vs high temperature\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "prompt = \"Write a one-line quote about teamwork.\"\n",
    "\n",
    "response_low = generator(prompt, temperature=0.2, max_new_tokens=30)\n",
    "response_high = generator(prompt, temperature=0.9, max_new_tokens=30)\n",
    "\n",
    "print(\"Low temperature:\", response_low[0][\"generated_text\"])\n",
    "print(\"High temperature:\", response_high[0][\"generated_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53fbed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 2 (Guided Practice)\n",
    "# Play with 'max_new_tokens'\n",
    "# 1. Generate a short version (20 tokens)\n",
    "# 2. Generate a longer version (80 tokens)\n",
    "# Observe the difference in length and tone.\n",
    "\n",
    "prompt = \"Describe a sunset.\"\n",
    "# Your code below üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ff8a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 2 (Critical Thinking)\n",
    "# Imagine you are designing a \"Headline Generator\".\n",
    "# You want short, catchy one-liners.\n",
    "# Which parameters should you adjust and why?\n",
    "# Try modifying the code above to reflect your idea.\n",
    "# Write your reasoning in comments üëá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04c06b",
   "metadata": {},
   "source": [
    "## üß© Concept 3: Choosing the Right Model\n",
    "\n",
    "**Theory Recap:**  \n",
    "Different models are trained for different purposes:\n",
    "- `flan-t5-small` ‚Üí instruction-following / Q&A  \n",
    "- `distilgpt2` ‚Üí text continuation  \n",
    "- `microsoft/DialoGPT-small` ‚Üí dialogue/chat  \n",
    "\n",
    "Each model has its own strengths.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d223f9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Compare FLAN vs DialoGPT on the same input\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Model 1: Instruction model\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "# Model 2: Dialogue model\n",
    "dialogpt = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-small\")\n",
    "\n",
    "prompt = \"How do I make a good first impression?\"\n",
    "\n",
    "print(\"FLAN says:\", flan(prompt)[0]['generated_text'])\n",
    "print(\"DialoGPT says:\", dialogpt(prompt)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f01c4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 3 (Guided Practice)\n",
    "# Try your own input and compare outputs.\n",
    "# 1. Choose a question or instruction.\n",
    "# 2. Generate outputs using both models.\n",
    "# 3. Note down how the tone or response style differs.\n",
    "\n",
    "# Your code below üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af388152",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 3 (Critical Thinking)\n",
    "# If you were building a 'Study Helper' bot to answer questions simply,\n",
    "# which model would you pick? Why?\n",
    "# Can you modify your code to only keep the model that fits best?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf2c72",
   "metadata": {},
   "source": [
    "## üß© Concept 4: Connecting to a Streamlit App\n",
    "\n",
    "**Theory Recap:**  \n",
    "Streamlit helps us build simple web UIs for our chatbot ‚Äî  \n",
    "students can type questions and see AI responses in real time.  \n",
    "\n",
    "We won‚Äôt build the full app here ‚Äî but let‚Äôs preview how the logic works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7a44c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Basic Streamlit chatbot (run later as .py)\n",
    "\n",
    "# Save this as llm_chatbot_app.py and run:\n",
    "# streamlit run llm_chatbot_app.py\n",
    "\n",
    "\"\"\"\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"Mini Chatbot Demo\")\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "user_input = st.text_input(\"Ask me something:\")\n",
    "if user_input:\n",
    "    response = generator(user_input, max_new_tokens=60)\n",
    "    st.write(\"Bot:\", response[0]['generated_text'])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d8755",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 4 (Guided Practice)\n",
    "# 1. In your Streamlit app file, add a sidebar slider for 'max_new_tokens'.\n",
    "# 2. Let the user control the answer length interactively.\n",
    "# 3. Test how the response changes for small vs large values.\n",
    "# (You don‚Äôt have to run Streamlit here, just plan the code.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111f9b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 4 (Critical Thinking)\n",
    "# Think about a new feature you‚Äôd add if you had more time.\n",
    "# Example ideas:\n",
    "# - A dropdown to choose between models\n",
    "# - A toggle for ‚Äúcreative‚Äù vs ‚Äúprecise‚Äù mode\n",
    "# - Saving previous chat responses\n",
    "# Write your idea below üëá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58647c92",
   "metadata": {},
   "source": [
    "## üß≠ Wrap-Up & Look-Ahead Reflection\n",
    "\n",
    "### üéì What You Learned Today\n",
    "- How to use the **Hugging Face pipeline** to connect prompts ‚Üí models  \n",
    "- How **parameters** like temperature, top-p, and tokens change model behavior  \n",
    "- How to pick the **right model** for a given task (Flan vs GPT vs DialoGPT)  \n",
    "- How a simple **Streamlit UI** turns code into an interactive chatbot  \n",
    "\n",
    "---\n",
    "\n",
    "üéØ **Challenge for the Curious:**  \n",
    "Write down one ‚Äúpain point‚Äù you noticed while testing your chatbot today.  \n",
    "What felt limited or frustrating ‚Äî and what would you love to improve if you could?\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Think About‚Ä¶\n",
    "1. Our chatbot only knows what‚Äôs inside its model ‚Äî it can‚Äôt answer about *your* documents or notes.  \n",
    "   - How could we make it read PDFs or data files and respond using that knowledge?  \n",
    "\n",
    "2. Today‚Äôs bot handles one message at a time.  \n",
    "   - What if you wanted several ‚Äúmini-bots‚Äù ‚Äî one to search, one to plan, one to answer ‚Äî all working together?  \n",
    "\n",
    "3. Our model always starts fresh ‚Äî it forgets previous questions.  \n",
    "   - How could a chatbot remember your last conversation or build on context?  \n",
    "\n",
    "4. Curious minds only üöÄ  \n",
    "   - Ever wondered how these models can be **fine-tuned** on your own data, or how voice assistants use them in real time?  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
