{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49caf25d",
   "metadata": {},
   "source": [
    "# Session 2 – Notebook 2: LLM Basics\n",
    "\n",
    "**Objectives:**\n",
    "- Learn how to use Hugging Face models with the `pipeline`.\n",
    "- Compare LLM responses with rule-based chatbot answers.\n",
    "- Understand the advantages and limitations of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d4529",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# LLM CHATBOT (using Hugging Face pipeline)\n",
    "\n",
    "# Import the Hugging Face pipeline utility\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a small instruction-following model\n",
    "# flan-t5-small is good for Q&A and simple instructions\n",
    "gen = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77138780",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Greeting\n",
    "\n",
    "user_input = \"hi\"\n",
    "print(\"User:\", user_input)\n",
    "\n",
    "# Ask the model to generate a response (limit to 30 new tokens for short answers)\n",
    "response = gen(user_input, max_new_tokens=30)\n",
    "\n",
    "print(\"LLM Bot:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e326238",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Open-ended question\n",
    "\n",
    "user_input = \"what is your name?\"\n",
    "print(\"User:\", user_input)\n",
    "\n",
    "response = gen(user_input, max_new_tokens=30)\n",
    "\n",
    "print(\"LLM Bot:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f72844",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example 3: A creative question (not possible in rule-based bot)\n",
    "\n",
    "user_input = \"Tell me a short story about a cat who learns to code.\"\n",
    "print(\"User:\", user_input)\n",
    "\n",
    "response = gen(user_input, max_new_tokens=60)\n",
    "\n",
    "print(\"LLM Bot:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ac7fb",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    "- Did the LLM understand \"what is your name?\" even though we didn’t code a rule?  \n",
    "- How does this compare to the rule-based bot from Notebook 1?  \n",
    "- What kind of tasks are **impossible** for a rule-based bot but possible for an LLM?  \n",
    "- What are the risks of using an LLM (e.g., making things up)?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4c184",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# FULL LLM CHATBOT (CLI style with loop)\n",
    "\n",
    "print(\"LLM Chatbot (type 'bye' to exit)\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "\n",
    "    if user_input.lower() == \"bye\":\n",
    "        print(\"Bot: Goodbye, see you soon!\")\n",
    "        break\n",
    "\n",
    "    if user_input:\n",
    "        # Generate a response from the model\n",
    "        response = gen(user_input, max_new_tokens=50)\n",
    "        print(\"Bot:\", response[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
