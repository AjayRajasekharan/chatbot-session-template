{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25475188",
   "metadata": {},
   "source": [
    "## üß≠ Wrap-Up & Look-Ahead Reflection\n",
    "\n",
    "### üéì What You Learned Today\n",
    "- How to use the **Hugging Face pipeline** to connect prompts ‚Üí models  \n",
    "- How **parameters** like temperature, top-p, and tokens change model behavior  \n",
    "- How to pick the **right model** for a given task (Flan vs GPT vs DialoGPT)  \n",
    "- How a simple **Streamlit UI** turns code into an interactive chatbot  \n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Think About‚Ä¶\n",
    "1. Our chatbot only knows what‚Äôs inside its model ‚Äî it can‚Äôt answer about *your* documents or notes.  \n",
    "   - How could we make it read PDFs or data files and respond using that knowledge?  \n",
    "     > *(Hint: this challenge leads to **Retrieval-Augmented Generation (RAG)** ‚Äî next session!)*  \n",
    "\n",
    "2. Today‚Äôs bot handles one message at a time.  \n",
    "   - What if you wanted several ‚Äúmini-bots‚Äù ‚Äî one to search, one to plan, one to answer ‚Äî all working together?  \n",
    "     > *(That‚Äôs the world of **Multi-Agent AI**, coming soon.)*  \n",
    "\n",
    "3. Our model always starts fresh ‚Äî it forgets previous questions.  \n",
    "   - How could a chatbot remember your last conversation or build on context?  \n",
    "     > *(You‚Äôll discover memory and state management when we combine RAG + agents.)*  \n",
    "\n",
    "4. Curious minds only üöÄ  \n",
    "   - Ever wondered how these models can be **fine-tuned** on your own data, or how voice assistants use them in real time?  \n",
    "     > *(That‚Äôs where advanced GenAI topics like fine-tuning and multi-modal inputs come in ‚Äî optional reading!)*  \n",
    "\n",
    "---\n",
    "\n",
    "üéØ **Challenge for the Curious:**  \n",
    "Write down one ‚Äúpain point‚Äù you noticed while testing your chatbot today.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b8142",
   "metadata": {},
   "source": [
    "# üß† In-Class Exercise ‚Äì Solution\n",
    "### Building Your First LLM Chatbot\n",
    "\n",
    "This notebook contains **completed examples and explanations** for each step.  \n",
    "Use it to review what we did in class, experiment with the code, and explore how different models behave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e051e5",
   "metadata": {},
   "source": [
    "## üß© Concept 1: The Hugging Face Pipeline\n",
    "\n",
    "**Theory Recap:**  \n",
    "A pipeline is like a ‚Äúready-made tool‚Äù that connects your text input to an AI model.  \n",
    "Instead of manually loading weights and tokenizers, we use a *pipeline* for common tasks such as summarization, translation, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb9a79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Create a simple pipeline and use it\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Choose your task and model\n",
    "task = \"text2text-generation\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Step 2: Create the pipeline\n",
    "generator = pipeline(task, model=model_name)\n",
    "\n",
    "# Step 3: Try it out\n",
    "response = generator(\"Summarize: Artificial intelligence helps automate tasks.\")\n",
    "print(\"Output:\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2a012",
   "metadata": {},
   "source": [
    "‚úÖ This is the foundation of any chatbot.\n",
    "- `pipeline()` connects your text prompt to a model.\n",
    "- `\"text2text-generation\"` means the model expects an instruction and outputs text (e.g., \"Summarize\", \"Translate\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad96a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 1 ‚Äì Solution\n",
    "# Use a different model - distilgpt2\n",
    "from transformers import pipeline\n",
    "\n",
    "task = \"text-generation\"\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "generator = pipeline(task, model=model_name)\n",
    "response = generator(\"Once upon a time, there was a student who\", max_new_tokens=40)\n",
    "print(response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08171166",
   "metadata": {},
   "source": [
    "üìù Here, `distilgpt2` continues text instead of following explicit commands.  \n",
    "Try changing the prompt ‚Äî notice how it just ‚Äúkeeps writing‚Äù instead of summarizing or translating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c48f60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 1 ‚Äì Solution\n",
    "# Trying an incorrect task-model combo\n",
    "from transformers import pipeline\n",
    "\n",
    "gen_wrong = pipeline(\"text2text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "try:\n",
    "    response = gen_wrong(\"Translate: Hello world to French.\")\n",
    "    print(response[0]['generated_text'])\n",
    "except Exception as e:\n",
    "    print(\"Error observed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55be15",
   "metadata": {},
   "source": [
    "üß© Using the wrong task type can cause errors or strange outputs.  \n",
    "Each model is trained differently ‚Äî some follow tasks, others just predict next words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6b138",
   "metadata": {},
   "source": [
    "## üß© Concept 2: Controlling Model Creativity\n",
    "\n",
    "**Recap:**  \n",
    "You can control how ‚Äúcreative‚Äù or ‚Äúfocused‚Äù your chatbot is using parameters:\n",
    "- **temperature** ‚Üí randomness (0 = predictable, 1 = creative)  \n",
    "- **top_p** ‚Üí diversity of words considered  \n",
    "- **max_new_tokens** ‚Üí output length  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a5194",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Comparing low vs high temperature\n",
    "prompt = \"Write a one-line quote about teamwork.\"\n",
    "\n",
    "response_low = generator(prompt, temperature=0.2, max_new_tokens=30)\n",
    "response_high = generator(prompt, temperature=0.9, max_new_tokens=30)\n",
    "\n",
    "print(\"Low temperature:\", response_low[0]['generated_text'])\n",
    "print(\"High temperature:\", response_high[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042200f8",
   "metadata": {},
   "source": [
    "üéØ Lower temperature = more focused and consistent.  \n",
    "Higher temperature = more random and imaginative.  \n",
    "Both can be useful ‚Äî it depends on your goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53b382",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 2 ‚Äì Solution\n",
    "prompt = \"Describe a sunset.\"\n",
    "\n",
    "short = generator(prompt, max_new_tokens=20)\n",
    "long = generator(prompt, max_new_tokens=80)\n",
    "\n",
    "print(\"Short version:\", short[0]['generated_text'])\n",
    "print(\"\\nLong version:\", long[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d24a56",
   "metadata": {},
   "source": [
    "üìù Increasing `max_new_tokens` makes the answer longer and more descriptive.  \n",
    "You can control the response size to fit your app‚Äôs purpose (short summaries vs long paragraphs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2731c78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 2 ‚Äì Solution\n",
    "prompt = \"Create a catchy headline about teamwork.\"\n",
    "creative = generator(prompt, temperature=0.9, max_new_tokens=20)\n",
    "focused = generator(prompt, temperature=0.3, max_new_tokens=20)\n",
    "\n",
    "print(\"Creative:\", creative[0]['generated_text'])\n",
    "print(\"Focused:\", focused[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7cb74d",
   "metadata": {},
   "source": [
    "üí¨ The ‚Äúcreative‚Äù setting gives varied headlines; the ‚Äúfocused‚Äù one sticks to predictable phrasing.  \n",
    "This balance is key for designing personality in chatbots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c8b5c",
   "metadata": {},
   "source": [
    "## üß© Concept 3: Choosing the Right Model\n",
    "\n",
    "**Recap:**  \n",
    "Different models have different personalities:\n",
    "- `flan-t5-small` ‚Üí follows instructions clearly  \n",
    "- `distilgpt2` ‚Üí continues text freely  \n",
    "- `DialoGPT-small` ‚Üí mimics human dialogue  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7403f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Compare FLAN vs DialoGPT\n",
    "from transformers import pipeline\n",
    "\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "dialogpt = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-small\")\n",
    "\n",
    "prompt = \"How do I make a good first impression?\"\n",
    "\n",
    "print(\"FLAN says:\", flan(prompt)[0]['generated_text'])\n",
    "print(\"\\nDialoGPT says:\", dialogpt(prompt)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892daab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 3 ‚Äì Solution\n",
    "prompt = \"How can students stay motivated while studying?\"\n",
    "\n",
    "flan_output = flan(prompt)\n",
    "gpt_output = dialogpt(prompt, max_new_tokens=60)\n",
    "\n",
    "print(\"FLAN:\", flan_output[0]['generated_text'])\n",
    "print(\"\\nDialoGPT:\", gpt_output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c99b9",
   "metadata": {},
   "source": [
    "‚ú® The difference is clear:\n",
    "- **FLAN** gives a structured, clear answer.  \n",
    "- **DialoGPT** responds casually, like a conversation.  \n",
    "Pick the model that matches your chatbot‚Äôs role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ddde0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 3 ‚Äì Solution\n",
    "# Choosing the best model for a \"Study Helper\" bot\n",
    "study_bot = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "response = study_bot(\"Explain gravity like I'm 10 years old.\")\n",
    "print(response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3dee90",
   "metadata": {},
   "source": [
    "üß† Instruction-following models like FLAN are ideal for teaching or explaining concepts.  \n",
    "This is how ‚Äústudy bots‚Äù or ‚Äúassistant bots‚Äù are usually built.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e590d4",
   "metadata": {},
   "source": [
    "## üß© Concept 4: Connecting to a Streamlit App\n",
    "\n",
    "**Recap:**  \n",
    "Streamlit turns your chatbot logic into an interactive web app ‚Äî  \n",
    "letting users type prompts and see real-time answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69789b18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example Streamlit Snippet (for reference)\n",
    "\"\"\"\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"Mini Chatbot Demo\")\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "user_input = st.text_input(\"Ask me something:\")\n",
    "if user_input:\n",
    "    response = generator(user_input, max_new_tokens=60)\n",
    "    st.write(\"Bot:\", response[0]['generated_text'])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7efa520",
   "metadata": {},
   "source": [
    "üí° Streamlit is not just for chatbots ‚Äî it‚Äôs used widely for quick AI demos, dashboards, and prototypes.  \n",
    "You‚Äôll build your own app version in the next exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be1dff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 4 ‚Äì Example Solution (pseudocode)\n",
    "\n",
    "\"\"\"\n",
    "Add a sidebar slider to control response length:\n",
    "\n",
    "max_tokens = st.sidebar.slider(\"Max new tokens\", 20, 200, 80, 10)\n",
    "response = generator(user_input, max_new_tokens=max_tokens)\n",
    "st.write(\"Bot:\", response[0][\"generated_text\"])\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
