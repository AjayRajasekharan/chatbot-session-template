{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b26590",
   "metadata": {},
   "source": [
    "# ğŸ§  In-Class Exercise: Building Your First LLM Chatbot\n",
    "\n",
    "Welcome!  \n",
    "This notebook is your hands-on lab for **Session 2 â€“ Introduction to LLM Chatbots**.  \n",
    "Youâ€™ll go step-by-step through concepts we discussed in class â€” pipelines, parameters, and model behavior â€” and try small experiments to understand how LLMs actually â€œthink.â€  \n",
    "\n",
    "ğŸ‘‰ Each section follows this pattern:\n",
    "- **Mini Concept** (theory recap)  \n",
    "- **Example Block** (run + observe)  \n",
    "- **Task Block** (guided exercise)  \n",
    "- **Challenge Block** (think deeper / explore)  \n",
    "\n",
    "Letâ€™s get started ğŸš€  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad1381",
   "metadata": {},
   "source": [
    "## ğŸ§© Concept 1: The Hugging Face Pipeline\n",
    "\n",
    "**Theory Recap:**  \n",
    "A pipeline is like a â€œready-made toolâ€ that connects your text input to an AI model.  \n",
    "Instead of manually loading weights and tokenizers, we use a *pipeline* for common tasks such as summarization, translation, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261d628",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# âœ… Example: Create a simple pipeline and use it\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Choose your task and model\n",
    "task = \"text2text-generation\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Step 2: Create the pipeline\n",
    "generator = pipeline(task, model=model_name)\n",
    "\n",
    "# Step 3: Try it out\n",
    "response = generator(\"Summarize: Artificial intelligence helps automate tasks.\")\n",
    "print(\"Output:\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0e016",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ§  TASK 1 (Guided Practice)\n",
    "# Use a different model - distilgpt2\n",
    "# 1. Change the task to \"text-generation\"\n",
    "# 2. Use model_name = \"distilgpt2\"\n",
    "# 3. Create your own prompt like \"Once upon a time...\"\n",
    "\n",
    "# Your code below ğŸ‘‡\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "task = \"text-generation\"\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "generator = pipeline(task, model=model_name)\n",
    "response = generator(\"Once upon a time, there was a student who\", max_new_tokens=40)\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f54210",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ’¡ CHALLENGE 1 (Critical Thinking)\n",
    "# What happens if you use the WRONG task for a model?\n",
    "# Try using \"text2text-generation\" instead of \"text-generation\" for distilgpt2.\n",
    "# Does it throw an error or produce something odd?\n",
    "# Write your observation in a comment below ğŸ‘‡\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "try:\n",
    "    wrong_combo = pipeline(\"text2text-generation\", model=\"distilgpt2\")\n",
    "    response = wrong_combo(\"Translate: Hello world to French.\")\n",
    "    print(response[0][\"generated_text\"])\n",
    "except Exception as e:\n",
    "    print(\"Error observed:\", e)\n",
    "\n",
    "# Observation:\n",
    "# It didnâ€™t translate or give a meaningful response â€” sometimes it throws an error.\n",
    "# distilgpt2 isnâ€™t trained to â€œfollow instructionsâ€, it just predicts the next word.\n",
    "# So it doesnâ€™t understand commands like â€œTranslateâ€ or â€œSummarizeâ€.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8efdf",
   "metadata": {},
   "source": [
    "## ğŸ§© Concept 2: Controlling Model Creativity\n",
    "\n",
    "**Theory Recap:**  \n",
    "Parameters like `temperature`, `top_p`, and `max_new_tokens` control how â€œcreativeâ€ or â€œfocusedâ€ the modelâ€™s output is.  \n",
    "- **Temperature**: randomness (0 = deterministic, 1 = more creative).  \n",
    "- **Top-p**: diversity of words considered.  \n",
    "- **Max new tokens**: how long the response can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4af881",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# âœ… Example: Comparing low vs high temperature\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "prompt = \"Write a one-line quote about teamwork.\"\n",
    "\n",
    "response_low = generator(prompt, temperature=0.2, max_new_tokens=30)\n",
    "response_high = generator(prompt, temperature=0.9, max_new_tokens=30)\n",
    "\n",
    "print(\"Low temperature:\", response_low[0][\"generated_text\"])\n",
    "print(\"High temperature:\", response_high[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53fbed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ§  TASK 2 (Guided Practice)\n",
    "# Play with 'max_new_tokens'\n",
    "# 1. Generate a short version (20 tokens)\n",
    "# 2. Generate a longer version (80 tokens)\n",
    "# Observe the difference in length and tone.\n",
    "\n",
    "prompt = \"Describe a sunset.\"\n",
    "# Your code below ğŸ‘‡\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "prompt = \"Describe a sunset.\"\n",
    "\n",
    "short = generator(prompt, max_new_tokens=20)\n",
    "long = generator(prompt, max_new_tokens=80)\n",
    "\n",
    "print(\"Short:\", short[0][\"generated_text\"])\n",
    "print(\"\\nLong:\", long[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ff8a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ’¡ CHALLENGE 2 (Critical Thinking)\n",
    "# Imagine you are designing a \"Headline Generator\".\n",
    "# You want short, catchy one-liners.\n",
    "# Which parameters should you adjust and why?\n",
    "# Try modifying the code above to reflect your idea.\n",
    "# Write your reasoning in comments ğŸ‘‡\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "prompt = \"Create a catchy headline about teamwork.\"\n",
    "\n",
    "creative = generator(prompt, temperature=0.9, max_new_tokens=20)\n",
    "focused  = generator(prompt, temperature=0.3, max_new_tokens=20)\n",
    "\n",
    "print(\"Creative:\", creative[0][\"generated_text\"])\n",
    "print(\"Focused:\", focused[0][\"generated_text\"])\n",
    "\n",
    "# Observation:\n",
    "# The â€œcreativeâ€ one gives more fun or unusual headlines,\n",
    "# sometimes less consistent but more interesting.\n",
    "# The â€œfocusedâ€ one sounds safe or repetitive.\n",
    "# So higher temperature = more imagination, lower = more accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04c06b",
   "metadata": {},
   "source": [
    "## ğŸ§© Concept 3: Choosing the Right Model\n",
    "\n",
    "**Theory Recap:**  \n",
    "Different models are trained for different purposes:\n",
    "- `flan-t5-small` â†’ instruction-following / Q&A  \n",
    "- `distilgpt2` â†’ text continuation  \n",
    "- `microsoft/DialoGPT-small` â†’ dialogue/chat  \n",
    "\n",
    "Each model has its own strengths.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d223f9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# âœ… Example: Compare FLAN vs DialoGPT on the same input\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Model 1: Instruction model\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "# Model 2: Dialogue model\n",
    "dialogpt = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-small\")\n",
    "\n",
    "prompt = \"How do I make a good first impression?\"\n",
    "\n",
    "print(\"FLAN says:\", flan(prompt)[0]['generated_text'])\n",
    "print(\"DialoGPT says:\", dialogpt(prompt)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f01c4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ§  TASK 3 (Guided Practice)\n",
    "# Try your own input and compare outputs.\n",
    "# 1. Choose a question or instruction.\n",
    "# 2. Generate outputs using both models.\n",
    "# 3. Note down how the tone or response style differs.\n",
    "\n",
    "# Your code below ğŸ‘‡\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "dialogpt = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-small\")\n",
    "\n",
    "prompt = \"How can students stay motivated while studying?\"\n",
    "\n",
    "flan_output  = flan(prompt)[0][\"generated_text\"]\n",
    "gpt_output   = dialogpt(prompt, max_new_tokens=60)[0][\"generated_text\"]\n",
    "\n",
    "print(\"FLAN:\", flan_output)\n",
    "print(\"\\nDialoGPT:\", gpt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af388152",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ’¡ CHALLENGE 3 (Critical Thinking)\n",
    "# If you were building a 'Study Helper' bot to answer questions simply,\n",
    "# which model would you pick? Why?\n",
    "# Can you modify your code to only keep the model that fits best?\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "study_bot = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "response  = study_bot(\"Explain gravity like I'm 10 years old.\")\n",
    "print(response[0][\"generated_text\"])\n",
    "\n",
    "# Observation:\n",
    "# flan-t5-small gives short, simple explanations â€” perfect for teaching.\n",
    "# DialoGPT sounds more casual or off-topic.\n",
    "# Instruction-tuned models like FLAN are best for educational bots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf2c72",
   "metadata": {},
   "source": [
    "## ğŸ§© Concept 4: Connecting to a Streamlit App\n",
    "\n",
    "**Theory Recap:**  \n",
    "Streamlit helps us build simple web UIs for our chatbot â€”  \n",
    "students can type questions and see AI responses in real time.  \n",
    "\n",
    "We wonâ€™t build the full app here â€” but letâ€™s preview how the logic works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7a44c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# âœ… Example: Basic Streamlit chatbot (run later as .py)\n",
    "\n",
    "# Save this as llm_chatbot_app.py and run in terminal:\n",
    "# streamlit run llm_chatbot_app.py\n",
    "\n",
    "\"\"\"\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"Mini Chatbot Demo\")\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "user_input = st.text_input(\"Ask me something:\")\n",
    "if user_input:\n",
    "    response = generator(user_input, max_new_tokens=60)\n",
    "    st.write(\"Bot:\", response[0]['generated_text'])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d8755",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ§  TASK 4 (Guided Practice)\n",
    "# 1. In your Streamlit app file, add a sidebar slider for 'max_new_tokens'.\n",
    "# 2. Let the user control the answer length interactively.\n",
    "# 3. Test how the response changes for small vs large values.\n",
    "# (You donâ€™t have to run Streamlit here, just plan the code.)\n",
    "\n",
    "# Save this as llm_chatbot_app.py and run in terminal:\n",
    "# streamlit run llm_chatbot_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"LLM Chatbot with Controls ğŸ¤–\")\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "user_input  = st.text_input(\"Ask me something:\")\n",
    "max_tokens  = st.sidebar.slider(\"Max new tokens\", 20, 200, 80, 10)\n",
    "temperature = st.sidebar.slider(\"Temperature\", 0.0, 1.0, 0.7, 0.1)\n",
    "\n",
    "if user_input:\n",
    "    response = generator(user_input, max_new_tokens=max_tokens, temperature=temperature)\n",
    "    st.write(\"**Bot:**\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111f9b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ’¡ CHALLENGE 4 (Critical Thinking)\n",
    "# Think about a new feature youâ€™d add if you had more time.\n",
    "# Example ideas:\n",
    "# - A dropdown to choose between models\n",
    "# - A toggle for â€œcreativeâ€ vs â€œpreciseâ€ mode\n",
    "# - Saving previous chat responses\n",
    "# Write your idea below ğŸ‘‡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58647c92",
   "metadata": {},
   "source": [
    "## ğŸ§­ Wrap-Up & Look-Ahead Reflection\n",
    "\n",
    "### ğŸ“ What You Learned Today\n",
    "- How to use the **Hugging Face pipeline** to connect prompts â†’ models  \n",
    "- How **parameters** like temperature, top-p, and tokens change model behavior  \n",
    "- How to pick the **right model** for a given task (Flan vs GPT vs DialoGPT)  \n",
    "- How a simple **Streamlit UI** turns code into an interactive chatbot  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ Think Aboutâ€¦\n",
    "1. Our chatbot only knows whatâ€™s inside its model â€” it canâ€™t answer about *your* documents or notes.  \n",
    "   - How could we make it read PDFs or data files and respond using that knowledge?  \n",
    "     > *(Hint: this challenge leads to **Retrieval-Augmented Generation (RAG)**!)*  \n",
    "\n",
    "2. Todayâ€™s bot handles one message at a time.  \n",
    "   - What if you wanted several â€œmini-botsâ€ â€” one to search, one to plan, one to answer â€” all working together?  \n",
    "     > *(Thatâ€™s the world of **Multi-Agent AI**)*  \n",
    "\n",
    "3. Our model always starts fresh â€” it forgets previous questions.  \n",
    "   - How could a chatbot remember your last conversation or build on context?  \n",
    "     > *(Youâ€™ll discover memory and state management when we combine RAG + agents.)*  \n",
    "\n",
    "4. Curious minds only ğŸš€  \n",
    "   - Ever wondered how these models can be **fine-tuned** on your own data, or how voice assistants use them in real time?  \n",
    "     > *(Thatâ€™s where advanced GenAI topics like fine-tuning and multi-modal inputs come in)*  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ¯ **Challenge for the Curious:**  \n",
    "Write down one â€œpain pointâ€ you noticed while testing your chatbot today.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
