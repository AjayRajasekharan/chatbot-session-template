{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b26590",
   "metadata": {},
   "source": [
    "# üß† In-Class Exercise: Building Your First LLM Chatbot\n",
    "\n",
    "Welcome!  \n",
    "This notebook is your hands-on lab for **Session 2 ‚Äì Introduction to LLM Chatbots**.  \n",
    "You‚Äôll go step-by-step through concepts we discussed in class ‚Äî pipelines, parameters, and model behavior ‚Äî and try small experiments to understand how LLMs actually ‚Äúthink.‚Äù  \n",
    "\n",
    "üëâ Each section follows this pattern:\n",
    "- **Mini Concept** (theory recap)  \n",
    "- **Example Block** (run + observe)  \n",
    "- **Task Block** (guided exercise)  \n",
    "- **Challenge Block** (think deeper / explore)  \n",
    "\n",
    "Let‚Äôs get started üöÄ  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad1381",
   "metadata": {},
   "source": [
    "## üß© Concept 1: The Hugging Face Pipeline\n",
    "\n",
    "**Theory Recap:**  \n",
    "A pipeline is like a ‚Äúready-made tool‚Äù that connects your text input to an AI model.  \n",
    "Instead of manually loading weights and tokenizers, we use a *pipeline* for common tasks such as summarization, translation, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261d628",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Create a simple pipeline and use it\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Choose your task and model\n",
    "task = \"text2text-generation\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Step 2: Create the pipeline\n",
    "generator = pipeline(task, model=model_name)\n",
    "\n",
    "# Step 3: Try it out\n",
    "response = generator(\"Summarize: Artificial intelligence helps automate tasks.\")\n",
    "print(\"Output:\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0e016",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 1 (Guided Practice)\n",
    "# Use a different model - distilgpt2\n",
    "# 1. Change the task to \"text-generation\"\n",
    "# 2. Use model_name = \"distilgpt2\"\n",
    "# 3. Create your own prompt like \"Once upon a time...\"\n",
    "\n",
    "# Your code below üëá\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "task = \"text-generation\"\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "generator = pipeline(task, model=model_name)\n",
    "response = generator(\"Once upon a time, there was a student who\", max_new_tokens=40)\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f54210",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 1 (Critical Thinking)\n",
    "# What happens if you use the WRONG task for a model?\n",
    "# Try using \"text2text-generation\" instead of \"text-generation\" for distilgpt2.\n",
    "# Does it throw an error or produce something odd?\n",
    "# Write your observation in a comment below üëá\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "try:\n",
    "    wrong_combo = pipeline(\"text2text-generation\", model=\"distilgpt2\")\n",
    "    response = wrong_combo(\"Translate: Hello world to French.\")\n",
    "    print(response[0][\"generated_text\"])\n",
    "except Exception as e:\n",
    "    print(\"Error observed:\", e)\n",
    "\n",
    "# Observation:\n",
    "# It didn‚Äôt translate or give a meaningful response ‚Äî sometimes it throws an error.\n",
    "# distilgpt2 isn‚Äôt trained to ‚Äúfollow instructions‚Äù, it just predicts the next word.\n",
    "# So it doesn‚Äôt understand commands like ‚ÄúTranslate‚Äù or ‚ÄúSummarize‚Äù.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8efdf",
   "metadata": {},
   "source": [
    "## üß© Concept 2: Controlling Model Creativity\n",
    "\n",
    "**Theory Recap:**  \n",
    "Parameters like `temperature`, `top_p`, and `max_new_tokens` control how ‚Äúcreative‚Äù or ‚Äúfocused‚Äù the model‚Äôs output is.  \n",
    "- **Temperature**: randomness (0 = deterministic, 1 = more creative).  \n",
    "- **Top-p**: diversity of words considered.  \n",
    "- **Max new tokens**: how long the response can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4af881",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Comparing low vs high temperature\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "prompt = \"Write a one-line quote about teamwork.\"\n",
    "\n",
    "response_low = generator(prompt, temperature=0.2, max_new_tokens=30)\n",
    "response_high = generator(prompt, temperature=0.9, max_new_tokens=30)\n",
    "\n",
    "print(\"Low temperature:\", response_low[0][\"generated_text\"])\n",
    "print(\"High temperature:\", response_high[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53fbed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 2 (Guided Practice)\n",
    "# Play with 'max_new_tokens'\n",
    "# 1. Generate a short version (20 tokens)\n",
    "# 2. Generate a longer version (80 tokens)\n",
    "# Observe the difference in length and tone.\n",
    "\n",
    "prompt = \"Describe a sunset.\"\n",
    "# Your code below üëá\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "prompt = \"Describe a sunset.\"\n",
    "\n",
    "short = generator(prompt, max_new_tokens=20)\n",
    "long = generator(prompt, max_new_tokens=80)\n",
    "\n",
    "print(\"Short:\", short[0][\"generated_text\"])\n",
    "print(\"\\nLong:\", long[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ff8a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 2 (Critical Thinking)\n",
    "# Imagine you are designing a \"Headline Generator\".\n",
    "# You want short, catchy one-liners.\n",
    "# Which parameters should you adjust and why?\n",
    "# Try modifying the code above to reflect your idea.\n",
    "# Write your reasoning in comments üëá\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "prompt = \"Create a catchy headline about teamwork.\"\n",
    "\n",
    "creative = generator(prompt, temperature=0.9, max_new_tokens=20)\n",
    "focused  = generator(prompt, temperature=0.3, max_new_tokens=20)\n",
    "\n",
    "print(\"Creative:\", creative[0][\"generated_text\"])\n",
    "print(\"Focused:\", focused[0][\"generated_text\"])\n",
    "\n",
    "# Observation:\n",
    "# The ‚Äúcreative‚Äù one gives more fun or unusual headlines,\n",
    "# sometimes less consistent but more interesting.\n",
    "# The ‚Äúfocused‚Äù one sounds safe or repetitive.\n",
    "# So higher temperature = more imagination, lower = more accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04c06b",
   "metadata": {},
   "source": [
    "## üß© Concept 3: Choosing the Right Model\n",
    "\n",
    "**Theory Recap:**  \n",
    "Different models are trained for different purposes:\n",
    "- `flan-t5-small` ‚Üí instruction-following / Q&A  \n",
    "- `distilgpt2` ‚Üí text continuation  \n",
    "- `microsoft/DialoGPT-small` ‚Üí dialogue/chat  \n",
    "\n",
    "Each model has its own strengths.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d223f9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Compare FLAN vs DialoGPT on the same input\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Model 1: Instruction model\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "# Model 2: Dialogue model\n",
    "dialogpt = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-small\")\n",
    "\n",
    "prompt = \"How do I make a good first impression?\"\n",
    "\n",
    "print(\"FLAN says:\", flan(prompt)[0]['generated_text'])\n",
    "print(\"DialoGPT says:\", dialogpt(prompt)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f01c4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 3 (Guided Practice)\n",
    "# Try your own input and compare outputs.\n",
    "# 1. Choose a question or instruction.\n",
    "# 2. Generate outputs using both models.\n",
    "# 3. Note down how the tone or response style differs.\n",
    "\n",
    "# Your code below üëá\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "dialogpt = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-small\")\n",
    "\n",
    "prompt = \"How can students stay motivated while studying?\"\n",
    "\n",
    "flan_output  = flan(prompt)[0][\"generated_text\"]\n",
    "gpt_output   = dialogpt(prompt, max_new_tokens=60)[0][\"generated_text\"]\n",
    "\n",
    "print(\"FLAN:\", flan_output)\n",
    "print(\"\\nDialoGPT:\", gpt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af388152",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 3 (Critical Thinking)\n",
    "# If you were building a 'Study Helper' bot to answer questions simply,\n",
    "# which model would you pick? Why?\n",
    "# Can you modify your code to only keep the model that fits best?\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "study_bot = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "response  = study_bot(\"Explain gravity like I'm 10 years old.\")\n",
    "print(response[0][\"generated_text\"])\n",
    "\n",
    "# Observation:\n",
    "# flan-t5-small gives short, simple explanations ‚Äî perfect for teaching.\n",
    "# DialoGPT sounds more casual or off-topic.\n",
    "# Instruction-tuned models like FLAN are best for educational bots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf2c72",
   "metadata": {},
   "source": [
    "## üß© Concept 4: Connecting to a Streamlit App\n",
    "\n",
    "**Theory Recap:**  \n",
    "Streamlit helps us build simple web UIs for our chatbot ‚Äî  \n",
    "students can type questions and see AI responses in real time.  \n",
    "\n",
    "We won‚Äôt build the full app here ‚Äî but let‚Äôs preview how the logic works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7a44c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Example: Basic Streamlit chatbot (run later as .py)\n",
    "\n",
    "# Save this as llm_chatbot_app.py and run in terminal:\n",
    "# streamlit run llm_chatbot_app.py\n",
    "\n",
    "\"\"\"\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"Mini Chatbot Demo\")\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "user_input = st.text_input(\"Ask me something:\")\n",
    "if user_input:\n",
    "    response = generator(user_input, max_new_tokens=60)\n",
    "    st.write(\"Bot:\", response[0]['generated_text'])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d8755",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 4 (Guided Practice)\n",
    "# 1. In your Streamlit app file, add a sidebar slider for 'max_new_tokens'.\n",
    "# 2. Let the user control the answer length interactively.\n",
    "# 3. Test how the response changes for small vs large values.\n",
    "# (You don‚Äôt have to run Streamlit here, just plan the code.)\n",
    "\n",
    "# Save this as llm_chatbot_app.py and run in terminal:\n",
    "# streamlit run llm_chatbot_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.title(\"LLM Chatbot with Controls ü§ñ\")\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "user_input  = st.text_input(\"Ask me something:\")\n",
    "max_tokens  = st.sidebar.slider(\"Max new tokens\", 20, 200, 80, 10)\n",
    "temperature = st.sidebar.slider(\"Temperature\", 0.0, 1.0, 0.7, 0.1)\n",
    "\n",
    "if user_input:\n",
    "    response = generator(user_input, max_new_tokens=max_tokens, temperature=temperature)\n",
    "    st.write(\"**Bot:**\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111f9b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° CHALLENGE 4 (Critical Thinking)\n",
    "# Think about a new feature you‚Äôd add if you had more time.\n",
    "# Example ideas:\n",
    "# - A dropdown to choose between models\n",
    "# - A toggle for ‚Äúcreative‚Äù vs ‚Äúprecise‚Äù mode\n",
    "# - Saving previous chat responses\n",
    "# Write your idea below üëá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58647c92",
   "metadata": {},
   "source": [
    "## üß≠ Wrap-Up & Look-Ahead Reflection\n",
    "\n",
    "### üéì What You Learned Today\n",
    "- How to use the **Hugging Face pipeline** to connect prompts ‚Üí models  \n",
    "- How **parameters** like temperature, top-p, and tokens change model behavior  \n",
    "- How to pick the **right model** for a given task (Flan vs GPT vs DialoGPT)  \n",
    "- How a simple **Streamlit UI** turns code into an interactive chatbot  \n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Think About‚Ä¶\n",
    "1. Our chatbot only knows what‚Äôs inside its model ‚Äî it can‚Äôt answer about *your* documents or notes.  \n",
    "   - How could we make it read PDFs or data files and respond using that knowledge?  \n",
    "     > *(Hint: this challenge leads to **Retrieval-Augmented Generation (RAG)**!)*  \n",
    "\n",
    "2. Today‚Äôs bot handles one message at a time.  \n",
    "   - What if you wanted several ‚Äúmini-bots‚Äù ‚Äî one to search, one to plan, one to answer ‚Äî all working together?  \n",
    "     > *(That‚Äôs the world of **Multi-Agent AI**)*  \n",
    "\n",
    "3. Our model always starts fresh ‚Äî it forgets previous questions.  \n",
    "   - How could a chatbot remember your last conversation or build on context?  \n",
    "     > *(You‚Äôll discover memory and state management when we combine RAG + agents.)*  \n",
    "\n",
    "4. Curious minds only üöÄ  \n",
    "   - Ever wondered how these models can be **fine-tuned** on your own data, or how voice assistants use them in real time?  \n",
    "     > *(That‚Äôs where advanced GenAI topics like fine-tuning and multi-modal inputs come in)*  \n",
    "\n",
    "---\n",
    "\n",
    "üéØ **Challenge for the Curious:**  \n",
    "Write down one ‚Äúpain point‚Äù you noticed while testing your chatbot today.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
